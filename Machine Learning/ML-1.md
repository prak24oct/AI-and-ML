# ğŸ“š Introduction to Machine Learning (ML) â€” Notes

![Machine Learning Mind Map](assests/mind-map.png)

---

## 1ï¸âƒ£ What is Data & Data Science?

### â¡ï¸ What is Data?

- **Data** = Collection of facts, such as numbers, words, measurements, observations.

### â¡ï¸ What is Data Science?

- **Data Science** = A multidisciplinary field combining **statistics, computer science, and domain knowledge** to extract insights from data.

### â¡ï¸ Who is a Data Scientist?

> A person better at statistics than any software engineer and better at software engineering than any statistician!

---

## 2ï¸âƒ£ What is Machine Learning?

### â¡ï¸ Simple Definition:

- **Machine Learning** = A method of data analysis that **automates model building**.
- It allows computers to **learn from data** without explicit programming for every task.

---

### â¡ï¸ Real-World Applications:

- ğŸ¯ **Fraud detection** in banks.
- ğŸŒ **Web search results** optimization.
- ğŸ“ˆ **Credit scoring** for loans.
- ğŸ›  **Predicting equipment failure** in factories.
- ğŸ¥ **Recommendation engines** on Netflix, YouTube.
- ğŸ–¼ **Image recognition** and ğŸ“§ **Email spam filtering**.

---

## 3ï¸âƒ£ Types of Machine Learning

### â¡ï¸ Main Categories:

| Type                       | Description                           | Example                   |
| -------------------------- | ------------------------------------- | ------------------------- |
| **Supervised Learning**    | Learn from labeled data.              | Spam Email Detection ğŸ“§   |
| **Unsupervised Learning**  | Find hidden patterns in data.         | Customer Segmentation ğŸ›’  |
| **Reinforcement Learning** | Learn by trial and error via rewards. | Robot learning to walk ğŸ¤– |

---

## 4ï¸âƒ£ Supervised Learning ğŸ”

### â¡ï¸ How It Works:

- Trains using **labeled data** (input + correct output).
- Learns by comparing predictions to correct answers.

**Example:**  
Email â†’ Model â†’ Label: Spam or Not Spam.

---

### â¡ï¸ Two Main Problems:

- **Classification** â¡ï¸ Categorize (e.g., Cat vs Dog ğŸ±ğŸ¶)
- **Regression** â¡ï¸ Predict continuous value (e.g., House Price Prediction ğŸ¡ğŸ’²)

---

### â¡ï¸ The Data Split Strategy:

- **Training Set** â†’ To train model.
- **Validation Set** â†’ To tune model hyperparameters.
- **Test Set** â†’ Final evaluation of model's performance.

âš¡ **Important:** After seeing test set results, **no more model adjustments** allowed!

---

## 5ï¸âƒ£ Overfitting vs Underfitting ğŸ§ 

| Concept          | Meaning                                                              | Visualize          |
| ---------------- | -------------------------------------------------------------------- | ------------------ |
| **Overfitting**  | Model memorizes training data but fails on new data (high variance). | Model learns noise |
| **Underfitting** | Model too simple to capture trends (high bias).                      | Model too dumb     |

**âœ… Ideal:** Good balance between bias and variance.

---

## 6ï¸âƒ£ Evaluation Metrics (for Classification)

### â¡ï¸ 1. Accuracy

**Formula:**  
\[
\textbf{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
\]

**âš¡ Problem:** Useless when classes are **imbalanced**.

ğŸ§  **Example:** 99 dogs ğŸ¶ + 1 cat ğŸ± â†’ Predicting all dogs = 99% accuracy but 0% meaningful!

---

### â¡ï¸ 2. Precision and Recall

| Metric        | Definition                            | Formula                                   |
| ------------- | ------------------------------------- | ----------------------------------------- |
| **Precision** | How many selected items are relevant? | \(\text{Precision} = \frac{TP}{TP + FP}\) |
| **Recall**    | How many relevant items are selected? | \(\text{Recall} = \frac{TP}{TP + FN}\)    |

Where:

- TP = True Positive
- FP = False Positive
- FN = False Negative

---

### â¡ï¸ 3. F1-Score

**Definition:**

- Harmonic mean of **precision** and **recall**.

**Formula:**  
\[
\textbf{F1 Score} = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\]

âœ… Use when you need a balance between precision and recall.

---

### â¡ï¸ 4. Confusion Matrix

A table showing:
| | Predicted Positive | Predicted Negative |
|----------------|---------------------|---------------------|
| **Actual Positive** | True Positive (TP) | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN) |

ğŸ“Š It gives a complete picture of classification results.

---

## 7ï¸âƒ£ Python Coding Examples ğŸ

### â¡ï¸ Train/Test Split Example:

```python
from sklearn.model_selection import train_test_split

# Example data
X = [[0], [1], [2], [3]]
y = [0, 1, 2, 3]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
```

---

### â¡ï¸ Supervised Learning (Classification) Example:

```python
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load example data
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Train a model
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)

# Evaluate
print(f"Accuracy: {accuracy_score(y_test, predictions):.2f}")
```

---

### â¡ï¸ Confusion Matrix, Precision, Recall, F1 Score Example:

```python
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Assume y_test and predictions from above
print(confusion_matrix(y_test, predictions))
print(f"Precision: {precision_score(y_test, predictions, average='macro'):.2f}")
print(f"Recall: {recall_score(y_test, predictions, average='macro'):.2f}")
print(f"F1 Score: {f1_score(y_test, predictions, average='macro'):.2f}")
```

---

## 8ï¸âƒ£ Practice Questions ğŸ¯

### â¡ï¸ Conceptual

1. **What happens if your model has very high accuracy but your dataset is extremely imbalanced?**

   - âœ¨ Answer: Accuracy becomes misleading; precision and recall are better indicators.

2. **What is the purpose of a validation set?**
   - âœ¨ Answer: To tune hyperparameters without touching the test set.

---

### â¡ï¸ Numerical

Given:

- TP = 90, FP = 10, FN = 30

Calculate:

1. **Precision**
   - \[
     Precision = \frac{90}{90+10} = 0.9
     \]
2. **Recall**
   - \[
     Recall = \frac{90}{90+30} = 0.75
     \]
3. **F1 Score**
   - \[
     F1 = 2 \times \frac{0.9 \times 0.75}{0.9 + 0.75} = 0.81
     \]

---

## 9ï¸âƒ£ Summary & Key Takeaways ğŸ“

- Machine Learning **automates learning** from data.
- Three main types: **Supervised**, **Unsupervised**, **Reinforcement** learning.
- Evaluation matters â€” **Accuracy** is not always enough; **Precision, Recall, F1** needed too!
- Watch out for **Overfitting** (too complex) and **Underfitting** (too simple).

---

# ğŸ¯ Final Tip:

Always check your model's behavior on **real-world data** â€” numbers alone never tell the full story!
