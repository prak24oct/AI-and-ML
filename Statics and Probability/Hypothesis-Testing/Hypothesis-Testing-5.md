# ğŸ“š Hypothesis Testing (Part 5)

---

# 1ï¸âƒ£ Two Sample Variance Test (F-Test)

### âœ¨ What is it?

- **Purpose**: To test the **equality of two variances** from different populations.
- **Also Used For**: Testing equality of means across multiple populations using **ANOVA**.

---

### ğŸ“ˆ Example: Machine A vs Machine B

- **Samples**:
  - Machine A: 8 samples, **standard deviation = 1.1**
  - Machine B: 5 samples, **variance = 11**
- **Hypotheses**:

  - **Hâ‚€**: Æ¡Â²â‚ = Æ¡Â²â‚‚ (Variances are equal)
  - **Hâ‚**: Æ¡Â²â‚ â‰  Æ¡Â²â‚‚ (Variances are different)

- **Important Rule**:
  > Always place the **higher variance in the numerator** during the F-test calculation!

---

# 2ï¸âƒ£ Types of Tests Overview ğŸ§ª

| **Samples**   | **Test Type**                                                           | **Examples**         |
| ------------- | ----------------------------------------------------------------------- | -------------------- |
| One Sample    | Z-test, T-test, One Proportion Test, One Variance Test                  |
| Two Samples   | Z-test, T-test, Paired T-test, Two Proportions Test, Two Variances Test |
| More Than Two | **ANOVA**                                                               | Analysis of variance |

---

# 3ï¸âƒ£ One Variance Test (Chi-Square)

### ğŸ“ What is it?

- **Chi-Square Test** used for:

  - Testing **population variance** against a specified value.
  - Testing **goodness of fit**.
  - Testing **independence** in contingency tables.

- **Statistic Used**: **Chi-square (Ï‡Â²)**

---

# 4ï¸âƒ£ Comparing Means: T-Test vs ANOVA

### âš¡ Two-Sample T-Test

- **When**: Comparing **two population means**.
- **Hypotheses**:
  - **Hâ‚€**: Î¼A = Î¼B
  - **Hâ‚**: Î¼A â‰  Î¼B

---

### ğŸŒŸ ANOVA (Analysis of Variance)

- **When**: Comparing **more than two means**.
- **Hypotheses**:
  - **Hâ‚€**: Î¼A = Î¼B = Î¼C = Î¼D = ... = Î¼k
  - **Hâ‚**: At least **one mean is different**.

---

### â— Why use ANOVA instead of multiple T-tests?

- Example: If comparing 4 groups:
  - Number of T-tests = 6
  - Confidence level drops:  
    \[
    (0.95)^6 = 0.735
    \]
    â” Only **73.5%** confidence (high error chance).
- **ANOVA** â” Only **one test** needed â” maintains 95% confidence.

---

# 5ï¸âƒ£ ANOVA Example: Comparing Machine Outputs

**Machines and Readings:**

- Machine 1: [150, 151, 152, 152, 151, 150]
- Machine 2: [153, 152, 148, 151, 149, 152]
- Machine 3: [156, 154, 155, 156, 157, 155]

---

### ğŸ¯ Understanding Variations:

- **Variation Between Treatments**: Differences between group means.
- **Variation Within Treatments (Error)**: Differences within each group.

> ğŸ“Œ Larger within-group variations â” Harder to detect differences across groups.

---

# 6ï¸âƒ£ Goodness of Fit Test (Chi-Square)

### ğŸ² Purpose:

- To check if observed data **matches a theoretical distribution**.

### ğŸ“Œ Hypotheses:

- **Hâ‚€**: Data follows the specified distribution.
- **Hâ‚**: Data does not follow.

---

### ğŸ§ª Example 1: Is the Coin Biased?

- A coin is flipped **100 times**.
- **Results**: 40 heads, 60 tails.
- **Alpha (Î±)**: 0.05
- **Test**: Right-tail Chi-square test.

---

### ğŸ‘• Example 2: T-shirt Sales Distribution

| Size        | Expected Proportion | Observed Sales | Expected Sales |
| ----------- | ------------------- | -------------- | -------------- |
| Small       | 0.1                 | 25             | 22.5           |
| Medium      | 0.2                 | 41             | 45             |
| Large       | 0.4                 | 91             | 90             |
| Extra Large | 0.3                 | 68             | 67.5           |

- **Hypotheses**:
  - **Hâ‚€**: Sales proportions match expectations.
  - **Hâ‚**: Sales proportions differ.

---

# 7ï¸âƒ£ Contingency Tables (Chi-Square Test for Independence)

### ğŸ“‹ Purpose:

- Test if **two categorical variables** are related.

---

### ğŸ›  Example 1: Smoking Status

|           | Smoker | Non-Smoker | Total |
| --------- | ------ | ---------- | ----- |
| Male      | 60     | 40         | 100   |
| Female    | 35     | 40         | 75    |
| **Total** | 95     | 80         | 175   |

---

### ğŸ›  Example 2: Shift vs Operator

|           | Operator 1 | Operator 2 | Operator 3 | Total |
| --------- | ---------- | ---------- | ---------- | ----- |
| Shift 1   | 22         | 26         | 23         | 71    |
| Shift 2   | 28         | 62         | 26         | 116   |
| Shift 3   | 72         | 22         | 66         | 160   |
| **Total** | 122        | 110        | 115        | 347   |

---

### ğŸ” Hypotheses:

- **Hâ‚€**: No relationship between rows and columns.
- **Hâ‚**: A relationship exists (type unspecified).

---

# 8ï¸âƒ£ Bonus: Tukeyâ€™s HSD Test (linked in your PDF)

### ğŸ§  What is Tukeyâ€™s HSD?

- After an **ANOVA** tells us that differences exist, **Tukeyâ€™s HSD (Honestly Significant Difference)** tells us **which groups differ**.
- **Post-hoc test** â” Used after rejecting ANOVAâ€™s null hypothesis.
- **Python Libraries**:
  - `scipy.stats.tukey_hsd`
  - Tutorials: [Statology Tukey Test](https://www.statology.org/tukey-test-python/)

---

# ğŸ“œ Formulas & Key Definitions

---

### â­ F-Test Statistic

\[
F = \frac{\text{Variance}\_1}{\text{Variance}\_2}
\]
(_Higher variance in numerator_)

---

### â­ Chi-Square Statistic

\[
\chi^2 = \sum \frac{(O - E)^2}{E}
\]

- O = Observed frequency
- E = Expected frequency

---

### â­ ANOVA Variances

- **Between Groups**:
  \[
  \text{MS}_{\text{Between}} = \frac{\text{SS}_{\text{Between}}}{\text{df}\_{\text{Between}}}
  \]

- **Within Groups**:
  \[
  \text{MS}_{\text{Within}} = \frac{\text{SS}_{\text{Within}}}{\text{df}\_{\text{Within}}}
  \]

---

# ğŸ“Š Data Visualization Techniques

- **ANOVA** â” Group means plotted.
- **Goodness of Fit** â” Bar graphs: observed vs expected.
- **Contingency Tables** â” Heatmaps or cross-tab charts.

---

# ğŸ§  Practice Questions

### 1. Conceptual

ğŸ‘‰ Why should higher variance be placed in the numerator for F-test?

- **Answer**: To ensure correct comparison with the F-distribution (right-skewed).

---

### 2. Numerical

ğŸ‘‰ Calculate F-statistic:

- Variance A = 8, Variance B = 2
- **Answer**:
  \[
  F = \frac{8}{2} = 4
  \]

---

### 3. Conceptual

ğŸ‘‰ Why not use multiple T-tests instead of ANOVA for 5 groups?

- **Answer**: Multiple tests inflate Type I error. ANOVA controls error rate.

---

### 4. Numerical

ğŸ‘‰ A coin flipped 100 times has 40 heads. Expected = 50 heads. What is the Ï‡Â² contribution?

- **Answer**:
  \[
  \chi^2 = \frac{(40-50)^2}{50} = 2
  \]

---

# âœ¨ Summary & Key Takeaways

- **F-Test** â” Compare **two variances**.
- **Chi-Square** â” Test **one variance**, **goodness of fit**, **independence**.
- **T-Test** â” Compare **two means**.
- **ANOVA** â” Compare **more than two means**.
- **Goodness of Fit** â” Only **right-tail test**.
- **Contingency Table Test** â” Check relationship between categorical variables.
- **Tukeyâ€™s HSD** â” Identify specifically **which groups differ** after ANOVA.

---

Perfect!  
You're asking for **two clean deliverables**:

- ğŸ“š First Part: Structured **theory notes** (already prepared âœ…)
- ğŸ›  Second Part: **All concepts converted into Python coding problems** separately as a **Jupyter Notebook-ready note** âœ…âœ…

---

### ğŸ“‹ Let's now build the second part:

# ğŸ›  Hypothesis Testing â€” Python Coding Problems for Jupyter Notebook

---

# 1ï¸âƒ£ Two Sample Variance Test (F-Test)

## Problem Statement:

You are given two sets of sample data from two machines. Test if their variances are significantly different at 90% confidence level.

## Python Code:

```python
import scipy.stats as stats

# Machine A
std_dev_machineA = 1.1
nA = 8

# Machine B
var_machineB = 11
nB = 5

# Step 1: Calculate variances
var_machineA = std_dev_machineA ** 2

# Step 2: Calculate F-statistic (higher variance on top)
F = var_machineB / var_machineA
df1 = nB - 1
df2 = nA - 1

# Step 3: Get p-value
p_value = 2 * min(stats.f.cdf(F, df1, df2), 1 - stats.f.cdf(F, df1, df2))

# Step 4: Conclusion
alpha = 0.10  # 90% confidence

print(f"F-statistic: {F:.4f}")
print(f"P-value: {p_value:.4f}")

if p_value < alpha:
    print("âœ… Reject Hâ‚€: Variances are different.")
else:
    print("âŒ Fail to Reject Hâ‚€: Variances are equal.")
```

---

# 2ï¸âƒ£ One Variance Test (Chi-Square)

## Problem Statement:

Suppose you expect the variance of a manufacturing process to be 4. You collect 20 samples and calculate sample variance = 5.5. Test if the variance is significantly different at 5% level.

## Python Code:

```python
# Chi-Square Test for One Variance

import scipy.stats as stats

sample_variance = 5.5
expected_variance = 4
n = 20

chi_square_stat = (n - 1) * sample_variance / expected_variance
df = n - 1

# Two-tailed test
p_value = 2 * min(stats.chi2.cdf(chi_square_stat, df), 1 - stats.chi2.cdf(chi_square_stat, df))

alpha = 0.05

print(f"Chi-Square Statistic: {chi_square_stat:.4f}")
print(f"P-value: {p_value:.4f}")

if p_value < alpha:
    print("âœ… Reject Hâ‚€: Variance is different.")
else:
    print("âŒ Fail to Reject Hâ‚€: Variance is not significantly different.")
```

---

# 3ï¸âƒ£ Goodness of Fit Test (Chi-Square)

## Problem Statement:

Flip a coin 100 times. You get 40 heads and 60 tails. Check if the coin is biased at 95% confidence.

## Python Code:

```python
# Goodness of Fit Test

import scipy.stats as stats

# Observed frequencies
observed = [40, 60]

# Expected frequencies (fair coin)
expected = [50, 50]

# Chi-square test
chi2_stat, p_value = stats.chisquare(f_obs=observed, f_exp=expected)

alpha = 0.05

print(f"Chi-Square Statistic: {chi2_stat:.4f}")
print(f"P-value: {p_value:.4f}")

if p_value < alpha:
    print("âœ… Reject Hâ‚€: The coin is biased.")
else:
    print("âŒ Fail to Reject Hâ‚€: The coin is not biased.")
```

---

# 4ï¸âƒ£ Contingency Table Test (Chi-Square Test for Independence)

## Problem Statement:

Check if there is any association between smoking status and gender.

## Python Code:

```python
# Contingency Table Test

import scipy.stats as stats

# Contingency Table
# Rows: Gender (Male, Female)
# Columns: Smoker, Non-Smoker
data = [[60, 40],  # Male
        [35, 40]]  # Female

chi2_stat, p_value, dof, expected = stats.chi2_contingency(data)

alpha = 0.05

print(f"Chi-Square Statistic: {chi2_stat:.4f}")
print(f"P-value: {p_value:.4f}")
print("Expected Frequencies Table:")
print(expected)

if p_value < alpha:
    print("âœ… Reject Hâ‚€: Smoking status and gender are related.")
else:
    print("âŒ Fail to Reject Hâ‚€: No relationship between smoking and gender.")
```

---

# 5ï¸âƒ£ ANOVA (One-Way ANOVA Test)

## Problem Statement:

You collected test scores from 3 different machines. Check if there is a significant difference in their mean outputs.

## Python Code:

```python
# One-Way ANOVA

import scipy.stats as stats

# Machine readings
machine1 = [150, 151, 152, 152, 151, 150]
machine2 = [153, 152, 148, 151, 149, 152]
machine3 = [156, 154, 155, 156, 157, 155]

# Perform ANOVA
F_stat, p_value = stats.f_oneway(machine1, machine2, machine3)

alpha = 0.05

print(f"F-statistic: {F_stat:.4f}")
print(f"P-value: {p_value:.4f}")

if p_value < alpha:
    print("âœ… Reject Hâ‚€: At least one machine has a different mean.")
else:
    print("âŒ Fail to Reject Hâ‚€: No significant difference in machine outputs.")
```

---

# 6ï¸âƒ£ Post-ANOVA: Tukeyâ€™s HSD Test

## Problem Statement:

After ANOVA, find out which specific machines differ.

## Python Code:

```python
# Tukey HSD Test

import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Combine data
data = machine1 + machine2 + machine3
labels = (['Machine1'] * len(machine1)) + (['Machine2'] * len(machine2)) + (['Machine3'] * len(machine3))

# Create a DataFrame
df = pd.DataFrame({'Score': data, 'Machine': labels})

# Perform Tukey's HSD
tukey = pairwise_tukeyhsd(endog=df['Score'], groups=df['Machine'], alpha=0.05)
print(tukey.summary())
```

---

# ğŸ“ Summary Table

| **Test**                | **Python Function**                               | **Package** |
| ----------------------- | ------------------------------------------------- | ----------- |
| F-Test                  | `scipy.stats.f.cdf()`                             | SciPy       |
| One Variance Chi-Square | `scipy.stats.chi2.cdf()`                          | SciPy       |
| Goodness of Fit         | `scipy.stats.chisquare()`                         | SciPy       |
| Contingency Table       | `scipy.stats.chi2_contingency()`                  | SciPy       |
| One-Way ANOVA           | `scipy.stats.f_oneway()`                          | SciPy       |
| Tukeyâ€™s HSD             | `statsmodels.stats.multicomp.pairwise_tukeyhsd()` | statsmodels |

---
